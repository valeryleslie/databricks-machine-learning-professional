{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "234f1eb2-b4c1-49f3-826f-6d723a522483",
      "cell_type": "markdown",
      "source": "# Databricks Certified Machine Learning Professional Notes",
      "metadata": {}
    },
    {
      "id": "c1b081e6-8cdd-48df-8c4f-ee01c7144db9",
      "cell_type": "markdown",
      "source": "## Experimentation",
      "metadata": {}
    },
    {
      "id": "94fe0ddd-f1ad-4df4-8042-0979d0829e68",
      "cell_type": "markdown",
      "source": "### Data Management",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      }
    },
    {
      "id": "3e3b7b4f-4f38-4264-8506-1f958c6763f1",
      "cell_type": "markdown",
      "source": "#### Delta Lake",
      "metadata": {}
    },
    {
      "id": "8a65e28b-3980-431b-8001-678f9321cb61",
      "cell_type": "markdown",
      "source": "##### Optimization\n\n**OPTIMIZE Command**\n\n`%sql OPTIMIZE table`\n\n**VACUUM Command**\n\n`spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\") # Retain 0 Hours of History`\n\n`%sql VACUUM table RETAIN 0 HOURS`\n\n**Z-Ordering Command**\n\n`%sql OPTIMIZE table ZORDER BY (col)`\n\n**Liquid Clustering: optimize for columns**\n\n`%sql ALTER TABLE table cluster by (cola, colb)`\n\n##### Operations\n| Operation | Sample |\n|----------|----------|\n| Read  | `spark.read.format(\"delta\").load(path)//.table(table).drop(\"a\", \"b\")`   |\n| Read - Streaming Data  | `spark.readStream.table(table)`|\n| Write - Overwrite  | `df.drop(\"a\").write.format(\"delta\").option(\"overwriteSchema\", \"True\").mode(\"overwrite\").save(path)//.saveAsTable(table)`  |\n| Write - Append  | `df.write.format(\"delta\").mode(\"append\").save(path)//.saveAsTable(table)`   |\n| Extracting History  | `spark.sql(\"DESCRIBE HISTORY table\")`   |\n| Extracting Timestamp  | `spark.sql(\"DESCRIBE HISTORY table\").orderBy(\"version\").first().timestamp`   |\n| Reading by Timestamp  | `spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp).table(table)`   |\n| Reading by Version  | `spark.read.format(\"delta\").option(\"versionAsOf\", 12).load(path)`   |",
      "metadata": {}
    },
    {
      "id": "1e919dde-352c-4191-a0f3-76fdddb17faf",
      "cell_type": "markdown",
      "source": "#### Feature Store",
      "metadata": {}
    },
    {
      "id": "2429d147-5617-41d7-a811-ec954b95724f",
      "cell_type": "markdown",
      "source": "**imports**\n\n```\nfrom databricks.feature_store import FeatureStoreClient, FeatureLookUp, FeatureFunction\nfrom databricks.feature_engineering import FeatureEngineeringClient # Feature Engineering Client alternative\nfs = FeatureStoreClient()\n```\n\n##### Create Table\n**Create Table by Dataframe**\n\n```\nfs.create_table(\n\tname = name, \n        df = df,\n        schema = schema, # Optional\n\tdescription = desc,\n\tprimary_keys = [],\n\ttimestamp_keys = [],\n\tpartition_columns = [])\n```\n\n**Create Table by Schema**\n\n```\nfs.create_table(\n    schema = schema,\n    description = desc,\n    primary_keys = [],\n    timestamp_keys = [],\n    partition_columns = []) # results in an empty table\n```\n##### Read Table\n**Read Table by Name**\n\n`fs.read_table(name = name)`\n\n**Read Table by Version**\n\n`ts = spark.sql(“DESCRIBE HISTORY name”).collect()[2].timestamp # Version 2`\n\n`fs.read_table(name = name, as_of_delta_timestamp = ts)`\n\n##### Write Table\n**Write Table - Merge**\n\n`df_new = df.withColumn(“score_group”, when((df.score<=25), “low”).otherwise(“etc”)) # create a new column`\n\n`fs.write_table(name = name, df = df.select(“id”, “score_group”), mode = “merge”)`\n\n**Write Table - Overwrite**\n```\ncols = [“clean”, “location”, “communication”] average then drop cols\nnew_df = df.withColumn(“ave_cols”, expr(“+”.join(cols)) / lit(len(cols))).drop(cols)\nfs.write_table(name = name, df = new_df, mode = “overwrite”)\n```\n* This operation will not change the existing table schema, the dropped columns will still exist but will populate with all-null values\n\n**Get Table Metadata**\n\n`fs.get_table(table_name).description // .path_data_sources // .features`\n\n**Delete Table** \n\n`fs.drop_table(table_name)`\n\n##### Machine Learning Operations\n\n**Feature Function Definition**\n\n```\n%sql\nCREATE OR REPLACE FUNCTION avg_func (a DOUBLE, b DOUBLE) RETURNS DOUBLE\nLANGUAGE PYTHON AS\n$$\navg = a / b\nreturn avg\n$$\n```\n\n**Feature LookUp Definition**\n```\nlookups=\t[FeatureLookup( table_name = table_name,\n                \t\tlookup_key = \"lookup_key\",\n                                timestamp_lookup_key = \"\"ts_key\",\n                                feature_names = []),\n                FeatureFunction(udf_name = “avg_func”,\n                                output_name = “avg”,\n                                input_bindings = {“a”: “a”, “b”: “b”})]\n```\n\n**Feature Training Set**\n```\ntraining_set = fs.create_training_set(  df = df, \n                                        feature_lookups = lookups, \n                                        label = “quality”, \n                                        exclude_columns = [\"id_column\"]) \ntraining_df = training_set.load_df()\n```\n\n**Logging a Model**\n```\nfs.log_model(   model = model, \n                registered_model_name = model_name, \n                training_set = training_set, \n                flavor = mlflow.sklearn, \n                artifact_path = artifact_path)\n```\n\n**Batch Scoring**\n```\nclient = MlflowClient()\nlatest_version = client.get_latest_version(model_name, stages=[“None”])[0].version\nfs.score_batch(f”models:/{model_name}/{latest_version}”, input_df, result_type=”string”)\n```\n",
      "metadata": {}
    },
    {
      "id": "e72a1e88-21ba-4f00-8937-76cfa6e0df75",
      "cell_type": "markdown",
      "source": "### Experiment Tracking",
      "metadata": {}
    },
    {
      "id": "05d43bfe-acfe-44b9-b522-d3261f9d441f",
      "cell_type": "markdown",
      "source": "**Basic Logging**\n```\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nexperiment_id = experiment.experiment_id\nwith mlflow.start_run(experiment_id = experiment_id, run_name = run_name) as run:\n\tmlflow.flavor.log_model(model, “artifact_path”)\n\tmlflow.log_metric(“rmse”, rmse)\n\tmlflow.log_param(“trees”, tree_num)\n\tmlflow.log_metrics({“mse”, mse})\n\tmlflow.log_params({})\n\tmlflow.log_artifact(csv_path, “feature-importance.csv”)\n\tmlflow.log_figure(fig, “feature_importance.png”)\n\tprint(run.info.run_id)\n```\n\n**Accessing Metadata**\n```\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\n```\n| Operation | sample |\n|----------|----------|\n| List Experiments   | `client.list_run_infos(experiment_id)`   |\n| Load Experiment Metadata - Spark DataFrame   | `spark.read.format(“mlflow-experiment”).load(experiment_id)`   |\n| Load Run Artifacts/Data   | `run_id = runs.orderBy(“start_time”, ascending = False).first().run_id`   |\n| Pull Run Metrics   | `client.get_run(run_id).data.metrics`   |\n| List Run Artifacts/Files   | `client.list_artifacts(run_id)`   |\n| Load Model   | `mlflow.flavor.load(f”runs:/{run_id}/model”)`   |\n\n**Register a Model with Signature**\n```\nfrom mlflow.models import infer_signature\nmlflow.register_model(model_uri, name)\nmlflow.log_model(model,\n\t\tartifact_path = …,\n\t\tsignature = infer_signature(..,..) // input_examples=..,\n\t\tregistered_model_name…)\n```\n\n**Nest Runs**\n```\nwith mlflow.start_run(run_name = …) as run:\n\twith mlflow.start_run(run_name = …, nested=True)\n```\n\nOR\n\n`with mlflow.start_run(run_name = …, parent_run_id = run.info.run_id):`\n\n**Autologging**\n\n`mlflow.flavor.autolog()`\n\n**SHAP Logging**\n\n`mlflow.shap.log_explanation()`",
      "metadata": {}
    },
    {
      "id": "fa64ef67-62aa-4f3d-bd92-ff25c18e4699",
      "cell_type": "markdown",
      "source": "## Model Lifecycle Management",
      "metadata": {}
    },
    {
      "id": "8aee7403-fe6c-402b-9ae1-9d0870fb06c8",
      "cell_type": "markdown",
      "source": "### Preprocessing Logic",
      "metadata": {}
    },
    {
      "id": "3d763250-62ca-4b78-8ec3-4f4e737278ca",
      "cell_type": "markdown",
      "source": "**PyFunc Model Definition**\n\n`class sklearn_model(mlflow.pyfunc.PythonModel)`\n\n**Model Metadata**\n```\nfrom mlflow.tracking.client import MLflowClient\nclient = MLflowClient()\n```\n\n`details = client.get_model_version(name=model_name, version=1)`\n\n`details = client.get_model_version_by_alias(name=model_name, alias=”Champion”)`\n\n`details.name//run_id//version//current_stage//tags//description//status//metrics//model_id//params//source`\n\n`reg_model_details = client.get_registered_model(‘name’)`\n\n`reg_model_details.name//aliases//description//latest_versions//tags//`\n\n**Transition Model Versions**\n```\nclient.transition_model_version_stage(\n        name = details.name,\n        version: str = details.version,\n        stage = “Production”)\n```\n\n**Verify Transition**\n```\nmodel_details = client.get_model_version(\n\tname = details.name,\n\tversion: str = details.version)\nmodel_details.current_stage ‘Production’\n```\n**Add Metadata to a Registered Model**\n```\nclient.update_registered_model(\n\tname = details.name,\n        description = “this model….”)\n```\n\n**Add Metadata to a Registered Model Version**\n```\nclient.update_model_version(\n    name = details.name,\n    version = details.version,\n    description = “this model….”)\n```\n\n**Delete Model Versions**\n\n`client.delete_model_version(“name”, version) # archive a model first`\n\n`client.delete_registered_model(“name”) `",
      "metadata": {}
    },
    {
      "id": "98e07d12-aa9a-4346-9022-741c36e9d14a",
      "cell_type": "markdown",
      "source": "### Model Lifecycle Automation ",
      "metadata": {}
    },
    {
      "id": "40e98b7d-6ba5-4eef-8a5b-8d3f9d65b791",
      "cell_type": "markdown",
      "source": "**http_request**\n\n```\nfrom mlflow.databricks.databricks_utils import get_databricks_host_creds, http_request\n# Create a Webhook\njob_json={  model_name = model,\n            events = [],\n            status = “”,\n            description = “”,\n            job_spec = {\n                    “job_id”: job_id, \n                    “workspace_url”: …, \n                    “access_token”: …}}\nhttp_request(\n    host_creds = get_databricks_host_creds(“databricks”)\n    endpoint = “/api/2.0/mlflow/registry-webhooks/create”,\n    method = “POST”,\n    json = job_json)\n# List Webhooks\nhttp_request(\t\n    host_creds = get_databricks_host_creds(“databricks”),\n    endpoint = “/api/2.0/mlflow/registry-webhooks/list/?model_name=model”\n    method = “GET”)\n# Delete Webhook\nhttp_request(\n    host_creds = get_databricks_host_creds(“databricks”),\n    endpoint = “/api/2.0/mlflow/registry-webhooks/delete”,\n    method = “DELETE”,\n    json = {“id”: id})\n```\n\n**RegistryWebhooksClient**\n\n```\n%sh pip install databricks-registry-webhooks\nfrom databricks_registry_webhooks import HttpUrlSpec, RegistryWebhooksClient\n# Create a Webhook\nhttp_url_spec = HttpUrlSpec(\n        url=\"https://hooks.slack.com/services/...\",\n        secret=\"secret_string\",\n        authorization=\"Bearer token\")\nRegistryWebhooksClient().create_webhook(\n                                        model_name = name,\n                                        description = “”,\n                                        events = [],\n                                        status = “”,\n                                        http_spec = http_url_spec)\n# List Webhooks\nRegistryWebhooksClient().list_webhooks(model_name=name)\n# Delete Webhook\nRegistryWebhooksClient().delete_webhook(id=id)\n# Update a Webhook Status\nRegistryWebhooksClient().update_webhook(id=id, status=status)\n```\n\n**Events**\n* REGISTERED_MODEL_CREATED\n* MODEL_VERSION_CREATED\n* COMMENT_CREATED\n* MODEL_VERSION_TAG_SET\n* MODEL_VERSION_TRANSITIONED_STAGE\n* MODEL_VERSION_TRANSITIONED_TO_XXX\n* TRANSITION_REQUEST_CREATED\n* TRANSITION_REQUEST_TO_XXX_CREATED\n\nXXX: STAGING/PRODUCTION/ARCHIVED\n\n**Statuses**\nACTIVE | DISABLED | TEST_MODE",
      "metadata": {}
    },
    {
      "id": "589b314d-5997-47df-b17f-de0ca94c070a",
      "cell_type": "markdown",
      "source": "## Model Deployment",
      "metadata": {}
    },
    {
      "id": "9a5b9bb9-8153-4c89-a137-afa47999c76c",
      "cell_type": "markdown",
      "source": "#### Batch",
      "metadata": {}
    },
    {
      "id": "c41a5e5c-5423-4d29-b01b-31ab17ac80ae",
      "cell_type": "markdown",
      "source": "**Load a Model**\n\n`mlflow.set_registry_uri(“databricks-uc”)`\n\n`mlflow.flavor.load_model(model_uri)`\n\n**Load a Single-Node Model with spark_udf**\n\n`mlflow.pyfunc.spark_udf(spark, model_uri, result_type = “”)`\n\n**Convert a Batch Deployment Pipeline to Streaming**\n\n```\nmlflow.set_registry_uri(“databricks-uc”)\nmlflow.pyfunc.spark_udf(spark, model_uri, result_type = “”)\n# DLT Inference\nimport dlt\nfrom pyspark.sql.functions import col, struct\n# Create 3 DLT tables...\n@dlt.table(name=”1”, comment=””, table_properties={})\n    ...\n@dlt.table(name=”2”, comment=””, table_properties={})\n    ...\n@dlt.table(name=”3”, comment=””, table_properties={})\n    ...\ndef raw_inputs():\n\treturn spark.read.csv(bronze_path, inferSchema=True, header=True, multiLine=True)\n```",
      "metadata": {}
    },
    {
      "id": "50d8e951-c11c-42a6-a9cb-c3f642837808",
      "cell_type": "markdown",
      "source": "#### Real-time",
      "metadata": {}
    },
    {
      "id": "a43f6e42-6485-4f87-b039-bfef5e475554",
      "cell_type": "markdown",
      "source": "**Create a Serving Endpoint**\n```\nfrom databricks.sdk import WorkspaceClient\nfrom mlflow.tracking import MlflowClient\nfrom databricks.sdk.service.serving import EndpointCoreConfigInput\nw = WorkspaceClient()\nclient = MlflowClient()\nfs_model_version = client.get_model_version_by_alias(model_name, alias = “Champion”).version\nfs_endpoint_config_dict = { “served_entities”: [{\t\n            “model_name”: model_name,\n\t\t\t“model_version” fs_model_version,\n\t\t\t“scale_to_zero_enabled”: True,\n\t\t\t“workload_size’: “Small”} ] }\nfs_endpoint_config = EndpointCoreConfigInput.from_dict(ds_endpoint_config_dict)\nw.serving_endpoints.create_and_wait(endpoint_name=endpoint_name, config=fs_endpoint_config)\n```\n**Query a Serving Endpoint**\n```\nw.serving_endpoints.query(name=endpoint_name, dataframe_records=df)\ndf[“prediction”] = query_response.predictions\ndf[“model”] = query_response.served_model_name\n```",
      "metadata": {}
    }
  ]
}